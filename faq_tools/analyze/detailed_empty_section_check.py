#!/usr/bin/env python3
"""
ç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚„è³ªå•æ•°ãŒå°‘ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è©³ç´°èª¿æŸ»
"""

import requests
import json
import os
import re
from dotenv import load_dotenv

load_dotenv()

def get_current_article(team_name, access_token, post_id):
    """ç¾åœ¨ã®è¨˜äº‹å†…å®¹ã‚’å–å¾—"""
    base_url = "https://api.docbase.io"
    headers = {
        "X-DocBaseToken": access_token,
        "Content-Type": "application/json"
    }
    
    url = f"{base_url}/teams/{team_name}/posts/{post_id}"
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def extract_product_sections(body):
    """å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’æŠ½å‡ºã—ã¦è©³ç´°åˆ†æ"""
    product_sections = {}
    
    # å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆçµµæ–‡å­—ä»˜ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
    product_pattern = r'## ([ğŸ§Šâ„ï¸ğŸ’¨ğŸ›ï¸ğŸ•ï¸ğŸ“¦ğŸ”‹ğŸ¦ºğŸ§£ğŸ§¤ğŸ“»âš¡]\s*[^#\n\r]+)'
    
    matches = list(re.finditer(product_pattern, body))
    
    for i, match in enumerate(matches):
        section_name = match.group(1).strip()
        start_pos = match.start()
        
        # æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¾ã§ã®å†…å®¹ã‚’å–å¾—
        if i + 1 < len(matches):
            end_pos = matches[i + 1].start()
        else:
            # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å ´åˆã€æ¬¡ã®ä¸»è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆç›®æ¬¡ä»¥å¤–ã®##ï¼‰ã¾ã§
            remaining_text = body[start_pos:]
            next_main_section = re.search(r'## [^ğŸ§Šâ„ï¸ğŸ’¨ğŸ›ï¸ğŸ•ï¸ğŸ“¦ğŸ”‹ğŸ¦ºğŸ§£ğŸ§¤ğŸ“»âš¡]', remaining_text)
            if next_main_section:
                end_pos = start_pos + next_main_section.start()
            else:
                end_pos = len(body)
        
        section_content = body[start_pos:end_pos]
        product_sections[section_name] = section_content
    
    return product_sections

def analyze_section(section_name, section_content):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ"""
    analysis = {
        'name': section_name,
        'total_length': len(section_content),
        'has_details_tag': '<details>' in section_content,
        'has_questions': False,
        'question_count': 0,
        'questions': [],
        'has_empty_message': False,
        'issues': []
    }
    
    # ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
    empty_patterns = [
        'ç¾åœ¨ã€ç‰¹å®šã®è³ªå•ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“',
        'ç¾åœ¨ã€ç‰¹å®šã®è³ªå•ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚',
        'ã‚ˆãã‚ã‚‹è³ªå•\n\nç¾åœ¨ã€ç‰¹å®šã®è³ªå•ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“',
        'ã‚ˆãã‚ã‚‹è³ªå•\r\n\r\nç¾åœ¨ã€ç‰¹å®šã®è³ªå•ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“'
    ]
    
    for pattern in empty_patterns:
        if pattern in section_content:
            analysis['has_empty_message'] = True
            analysis['issues'].append(f"ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: '{pattern}'")
            break
    
    # è³ªå•ã‚’æŠ½å‡º
    question_pattern = r'#### Q:\s*([^\n\r]+)'
    questions = re.findall(question_pattern, section_content)
    
    analysis['question_count'] = len(questions)
    analysis['questions'] = questions
    analysis['has_questions'] = len(questions) > 0
    
    # å•é¡Œã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
    if analysis['has_empty_message']:
        analysis['issues'].append("è³ªå•ãŒç©ºã«ãªã£ã¦ã„ã¾ã™")
    elif analysis['question_count'] == 0 and analysis['has_details_tag']:
        analysis['issues'].append("detailsã‚¿ã‚°ã¯ã‚ã‚‹ãŒè³ªå•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    elif analysis['question_count'] < 2 and not analysis['has_empty_message']:
        analysis['issues'].append(f"è³ªå•æ•°ãŒå°‘ãªã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ ({analysis['question_count']}å€‹)")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
    if analysis['total_length'] < 200:
        analysis['issues'].append("ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†…å®¹ãŒçŸ­ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    
    return analysis

def main():
    TEAM_NAME = "go"
    POST_ID = 2705590
    ACCESS_TOKEN = os.getenv("DOCBASE_ACCESS_TOKEN")
    
    if not ACCESS_TOKEN:
        print("ç’°å¢ƒå¤‰æ•° DOCBASE_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    print("ğŸ” ç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨è³ªå•æ•°ã®è©³ç´°èª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 70)
    
    # ç¾åœ¨ã®è¨˜äº‹ã‚’å–å¾—
    current_article = get_current_article(TEAM_NAME, ACCESS_TOKEN, POST_ID)
    if not current_article:
        print("âŒ ç¾åœ¨ã®è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    current_body = current_article['body']
    
    # å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    product_sections = extract_product_sections(current_body)
    print(f"ğŸ“¦ å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(product_sections)}")
    print()
    
    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†æ
    problematic_sections = []
    empty_sections = []
    low_question_sections = []
    
    for section_name, section_content in product_sections.items():
        analysis = analyze_section(section_name, section_content)
        
        # å•é¡ŒãŒã‚ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†é¡
        if analysis['issues']:
            problematic_sections.append(analysis)
            
            if analysis['has_empty_message']:
                empty_sections.append(analysis)
            elif analysis['question_count'] < 3 and not analysis['has_empty_message']:
                low_question_sections.append(analysis)
    
    # çµæœã®è¡¨ç¤º
    print("ğŸ“Š ã€èª¿æŸ»çµæœã€‘")
    print(f"   ç·å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(product_sections)}")
    print(f"   å•é¡ŒãŒã‚ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(problematic_sections)}")
    print(f"   å®Œå…¨ã«ç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(empty_sections)}")
    print(f"   è³ªå•æ•°ãŒå°‘ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(low_question_sections)}")
    print()
    
    if empty_sections:
        print("ğŸš¨ ã€å®Œå…¨ã«ç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘")
        for analysis in empty_sections:
            print(f"   âŒ {analysis['name']}")
            for issue in analysis['issues']:
                print(f"      â†’ {issue}")
        print()
    
    if low_question_sections:
        print("âš ï¸  ã€è³ªå•æ•°ãŒå°‘ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘")
        for analysis in low_question_sections:
            print(f"   ğŸ“ {analysis['name']} ({analysis['question_count']}å€‹ã®è³ªå•)")
            if analysis['questions']:
                for i, q in enumerate(analysis['questions'], 1):
                    print(f"      {i}. {q}")
            for issue in analysis['issues']:
                print(f"      â†’ {issue}")
        print()
    
    # æ­£å¸¸ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆ
    normal_sections = [s for s in product_sections.items() 
                      if not any(analysis['name'] == s[0] for analysis in problematic_sections)]
    
    if normal_sections:
        question_counts = []
        for section_name, section_content in normal_sections:
            questions = re.findall(r'#### Q:', section_content)
            question_counts.append(len(questions))
        
        if question_counts:
            avg_questions = sum(question_counts) / len(question_counts)
            print(f"ğŸ“ˆ ã€æ­£å¸¸ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆã€‘")
            print(f"   å¹³å‡è³ªå•æ•°: {avg_questions:.1f}å€‹")
            print(f"   è³ªå•æ•°ç¯„å›²: {min(question_counts)}ã€œ{max(question_counts)}å€‹")
            print()
    
    # ç‰¹å®šã®èª¿æŸ»ï¼šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨æ¯”è¼ƒã—ã¦å†…å®¹ãŒå¤§å¹…ã«ç•°ãªã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    print("ğŸ” ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã®æ¯”è¼ƒèª¿æŸ»ã€‘")
    try:
        with open('article_backup.json', 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        backup_body = backup_data['body']
        
        backup_sections = extract_product_sections(backup_body)
        
        print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(backup_sections)}")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ã®é•ã„ã‚’ãƒã‚§ãƒƒã‚¯
        missing_in_current = set(backup_sections.keys()) - set(product_sections.keys())
        missing_in_backup = set(product_sections.keys()) - set(backup_sections.keys())
        
        if missing_in_current:
            print(f"   ç¾åœ¨ã®è¨˜äº‹ã«ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {len(missing_in_current)}å€‹")
            for section in missing_in_current:
                print(f"      â€¢ {section}")
        
        if missing_in_backup:
            print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {len(missing_in_backup)}å€‹")
            for section in missing_in_backup:
                print(f"      â€¢ {section}")
        
        # é•·ã•ãŒå¤§å¹…ã«ç•°ãªã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        length_differences = []
        for section_name in set(backup_sections.keys()) & set(product_sections.keys()):
            backup_len = len(backup_sections[section_name])
            current_len = len(product_sections[section_name])
            
            # 50%ä»¥ä¸Šã®å·®ãŒã‚ã‚‹å ´åˆ
            if backup_len > 0:
                diff_ratio = abs(current_len - backup_len) / backup_len
                if diff_ratio > 0.5:
                    length_differences.append({
                        'name': section_name,
                        'backup_len': backup_len,
                        'current_len': current_len,
                        'diff_ratio': diff_ratio
                    })
        
        if length_differences:
            print(f"   å¤§å¹…ã«é•·ã•ãŒå¤‰ã‚ã£ãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³: {len(length_differences)}å€‹")
            for diff in sorted(length_differences, key=lambda x: x['diff_ratio'], reverse=True):
                print(f"      â€¢ {diff['name']}")
                print(f"        ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {diff['backup_len']}æ–‡å­—")
                print(f"        ç¾åœ¨: {diff['current_len']}æ–‡å­—")
                print(f"        å¤‰åŒ–ç‡: {diff['diff_ratio']:.1%}")
        
    except FileNotFoundError:
        print("   âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    print()
    print("=" * 70)
    print("âœ… è©³ç´°èª¿æŸ»ãŒå®Œäº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()