#!/usr/bin/env python3
"""
å…¨å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è³ªå•ã‚’è©³ç´°æ¯”è¼ƒã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import requests
import json
import os
import re
from dotenv import load_dotenv

load_dotenv()

def get_current_article(team_name, access_token, post_id):
    """ç¾åœ¨ã®è¨˜äº‹å†…å®¹ã‚’å–å¾—"""
    base_url = "https://api.docbase.io"
    headers = {
        "X-DocBaseToken": access_token,
        "Content-Type": "application/json"
    }
    
    url = f"{base_url}/teams/{team_name}/posts/{post_id}"
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def extract_questions_from_section(section_text):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰Q&Aã‚’æŠ½å‡º"""
    questions = []
    
    # #### Q: ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è³ªå•ã‚’æ¤œç´¢
    q_pattern = r'#### Q:\s*([^\n\r]+)'
    a_pattern = r'\*\*A:\*\*\s*([^#]*?)(?=####|</details>|$)'
    
    q_matches = re.findall(q_pattern, section_text)
    
    for i, question in enumerate(q_matches):
        # å¯¾å¿œã™ã‚‹å›ç­”ã‚’æ¢ã™
        q_pos = section_text.find(f'#### Q: {question}')
        if q_pos != -1:
            remaining_text = section_text[q_pos:]
            a_match = re.search(r'\*\*A:\*\*\s*([^#]*?)(?=####|</details>|$)', remaining_text, re.DOTALL)
            if a_match:
                answer = a_match.group(1).strip()
                questions.append({
                    'question': question.strip(),
                    'answer': answer.strip()
                })
    
    return questions

def extract_all_sections(body):
    """è¨˜äº‹ã‹ã‚‰å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    sections = {}
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¢ƒç•Œã‚’ç‰¹å®šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    section_pattern = r'## ([^#\n\r]+)'
    section_matches = list(re.finditer(section_pattern, body))
    
    for i, match in enumerate(section_matches):
        section_name = match.group(1).strip()
        start_pos = match.start()
        
        # æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹ä½ç½®ã‚’å–å¾—
        if i + 1 < len(section_matches):
            end_pos = section_matches[i + 1].start()
        else:
            end_pos = len(body)
        
        section_content = body[start_pos:end_pos]
        sections[section_name] = section_content
    
    return sections

def compare_sections(backup_sections, current_sections):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã§è³ªå•æ•°ã¨å†…å®¹ã‚’æ¯”è¼ƒ"""
    comparison_results = {}
    
    # å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    product_keywords = [
        'ICEBERG', 'PowerArQ', 'Wearable', 'GearBox', 'PowerBank', 'Electric',
        'FM', 'ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒ¼ãƒ©ãƒ¼', 'å†·å´ãƒ™ã‚¹ãƒˆ', 'ãƒãƒ¼ã‚¿ãƒ–ãƒ«ç™ºé›»æ©Ÿ', 'ã‚·ã‚§ãƒ©ã‚«ãƒƒãƒ—'
    ]
    
    for section_name in backup_sections:
        # å•†å“é–¢é€£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯
        is_product_section = any(keyword in section_name for keyword in product_keywords)
        if not is_product_section:
            continue
            
        backup_content = backup_sections[section_name]
        current_content = current_sections.get(section_name, '')
        
        # è³ªå•ã‚’æŠ½å‡º
        backup_questions = extract_questions_from_section(backup_content)
        current_questions = extract_questions_from_section(current_content)
        
        # æ¯”è¼ƒçµæœã‚’è¨˜éŒ²
        result = {
            'section_name': section_name,
            'backup_count': len(backup_questions),
            'current_count': len(current_questions),
            'backup_questions': backup_questions,
            'current_questions': current_questions,
            'missing_questions': [],
            'has_changes': False
        }
        
        # å‰Šé™¤ã•ã‚ŒãŸè³ªå•ã‚’ç‰¹å®š
        backup_q_texts = [q['question'] for q in backup_questions]
        current_q_texts = [q['question'] for q in current_questions]
        
        for backup_q in backup_questions:
            if backup_q['question'] not in current_q_texts:
                result['missing_questions'].append(backup_q)
                result['has_changes'] = True
        
        # è³ªå•æ•°ãŒå¤‰åŒ–ã—ã¦ã„ã‚‹å ´åˆã¯è¨˜éŒ²
        if len(backup_questions) != len(current_questions) or result['has_changes']:
            comparison_results[section_name] = result
    
    return comparison_results

def main():
    TEAM_NAME = "go"
    POST_ID = 2705590
    ACCESS_TOKEN = os.getenv("DOCBASE_ACCESS_TOKEN")
    
    if not ACCESS_TOKEN:
        print("ç’°å¢ƒå¤‰æ•° DOCBASE_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    print("ğŸ” å…¨å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°æ¯”è¼ƒèª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    try:
        with open('article_backup.json', 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        backup_body = backup_data['body']
    except FileNotFoundError:
        print("âŒ article_backup.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # ç¾åœ¨ã®è¨˜äº‹ã‚’å–å¾—
    current_article = get_current_article(TEAM_NAME, ACCESS_TOKEN, POST_ID)
    if not current_article:
        print("âŒ ç¾åœ¨ã®è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    current_body = current_article['body']
    
    print("ğŸ“Š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºä¸­...")
    backup_sections = extract_all_sections(backup_body)
    current_sections = extract_all_sections(current_body)
    
    print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {len(backup_sections)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
    print(f"ç¾åœ¨ã®è¨˜äº‹: {len(current_sections)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
    print()
    
    print("ğŸ” è³ªå•ã®æ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")
    comparison_results = compare_sections(backup_sections, current_sections)
    
    # çµæœã®è¡¨ç¤º
    if not comparison_results:
        print("âœ… å‰Šé™¤ã•ã‚ŒãŸè³ªå•ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print(f"âš ï¸  {len(comparison_results)} å€‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å¤‰æ›´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ:")
    print("=" * 60)
    
    total_missing = 0
    problematic_sections = []
    
    for section_name, result in comparison_results.items():
        print(f"\nğŸ“¦ ã€{section_name}ã€‘")
        print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {result['backup_count']} è³ªå•")
        print(f"   ç¾åœ¨ã®è¨˜äº‹:   {result['current_count']} è³ªå•")
        
        if result['missing_questions']:
            print(f"   âŒ å‰Šé™¤ã•ã‚ŒãŸè³ªå•: {len(result['missing_questions'])} å€‹")
            total_missing += len(result['missing_questions'])
            problematic_sections.append(section_name)
            
            for i, missing_q in enumerate(result['missing_questions'], 1):
                print(f"      {i}. {missing_q['question']}")
                # å›ç­”ã®ä¸€éƒ¨ã‚‚è¡¨ç¤ºï¼ˆé•·ã„å ´åˆã¯çœç•¥ï¼‰
                answer_preview = missing_q['answer'][:100]
                if len(missing_q['answer']) > 100:
                    answer_preview += "..."
                print(f"         â†’ {answer_preview}")
        else:
            print("   âœ… å‰Šé™¤ã•ã‚ŒãŸè³ªå•ã¯ã‚ã‚Šã¾ã›ã‚“")
    
    print()
    print("=" * 60)
    print(f"ğŸ“ˆ ã€èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼ã€‘")
    print(f"   - èª¿æŸ»å¯¾è±¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {len(comparison_results)} å€‹")
    print(f"   - å•é¡Œã®ã‚ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {len(problematic_sections)} å€‹")
    print(f"   - å‰Šé™¤ã•ã‚ŒãŸè³ªå•ã®ç·æ•°: {total_missing} å€‹")
    
    if problematic_sections:
        print(f"\nğŸš¨ å¾©å…ƒãŒå¿…è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³:")
        for section in problematic_sections:
            print(f"   â€¢ {section}")
    
    # è©³ç´°çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open('comparison_results.json', 'w', encoding='utf-8') as f:
        json.dump(comparison_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©³ç´°ãªæ¯”è¼ƒçµæœã‚’ comparison_results.json ã«ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()